{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","from langchain_community.tools.tavily_search import TavilySearchResults\n","from dotenv import load_dotenv\n","from supabase import create_client\n","from langgraph.checkpoint.sqlite import SqliteSaver\n","from langchain_core.output_parsers import StrOutputParser\n","from typing import Annotated\n","from typing_extensions import TypedDict\n","from langgraph.graph import StateGraph, START, END\n","from langgraph.graph.message import add_messages\n","from langchain.schema import Document\n","from langgraph.prebuilt import ToolNode, tools_condition\n","from typing import Literal\n","from typing import List\n","from typing_extensions import TypedDict\n","from langchain.tools.retriever import create_retriever_tool\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.pydantic_v1 import BaseModel, Field\n","from langchain_openai import ChatOpenAI\n","from langgraph.checkpoint.memory import MemorySaver\n","from pprint import pprint\n","from typing import Any,  Literal, Union\n","from langchain_core.messages import  AnyMessage\n","import logging\n","import os"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Supabase client initialized: <supabase._sync.client.SyncClient object at 0x00000282FD219DC0>\n"]}],"source":["import supabase_service"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import tools_parra"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["load_dotenv()\n","DB_CONNECTION = os.getenv(\"DB_CONNECTION\")\n","OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n","SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n","SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n","TABLE_NAME = os.getenv(\"TABLE_NAME\")\n","LANGCHAIN_PROJECT = os.getenv(\"LANGCHAIN_PROJECT\")\n","LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n","LANGCHAIN_TRACING_V2=os.getenv(\"LANGCHAIN_TRACING_V2\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n","# Crear cliente de Supabase\n","supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n","embeddings = OpenAIEmbeddings()  # Inicializar embeddings\n","vector_store = supabase_service.load_vector_store()\n","retriever=vector_store.as_retriever(search_kwargs={\"k\":4})\n","memory = SqliteSaver.from_conn_string(\":memory:\") #despues se conecta a bd"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["retriever_tool=create_retriever_tool( \n","    retriever, \n","    'retrieve_info',\n","    ' Busca y devuelve informacion sobre los vehiculos del concesionario, repuestos e informacion general'\n","    ,document_separator=\"\\n\\n\\n\"\n",")   \n","\n","\n","web_search_tool = TavilySearchResults(max_results=2)\n","tools = [web_search_tool,tools_parra.tool_create_event_test_drive]\n","llm_tools=[retriever_tool,web_search_tool,tools_parra.tool_create_event_test_drive]"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["\n","#LLM with function call\n","llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n","agent= llm.bind_tools(llm_tools)\n","\n","#Prompt\n","system = \"\"\"Eres un asistente de servicio al cliente del concesionario Parra arango. \n","Debes comunicarte amablemente con el usuario y mantener precisa y concisa la conversación.\n","Tambien debes identificar cuando haya una llamda a una herramienta correctamente. Si el usuario pide un test drive, \n","recuerda agregar a las fechas el formato UTC, por ejemplo : '2015-05-28T09:00:00-05:00'\n","\"\"\"\n","route_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system),\n","        (\"human\", \"{input}\"),\n","    ]\n",")\n","\n","agent_router=route_prompt | agent\n","\n","### Retrieval Grader\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n","\n","# Data model\n","class GradeDocuments(BaseModel):\n","    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n","\n","    binary_score: str = Field(\n","        description=\"Documentos son relevante para la pregunta, 'si' o 'no'\"\n","    )\n","structured_llm_grader=llm.with_structured_output(GradeDocuments)\n","\n","# Prompt\n","system = \"\"\"\n","Usted es un evaluador que está valorando la relevancia de un documento recuperado respecto a una pregunta del usuario.\n","Si el documento contiene palabra(s) clave o un significado semántico relacionado con la pregunta del usuario, califíquelo como relevante.El objetivo es filtrar recuperaciones erróneas.\n","Asigne una puntuación binaria 'si' o 'no' para indicar si el documento es relevante para la pregunta.\n","\"\"\"\n","grade_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system),\n","        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n","    ]\n",")\n","retrieval_grader = grade_prompt | structured_llm_grader\n","# question = \"cual es el precio de la torres supreme 2025\"\n","# docs = retriever.invoke(question)\n","# doc_txt = docs[1].page_content\n","# print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n","\n","\n","### Generate\n","#llm \n","llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n","\n","system=\"\"\"\n","Eres un analista experto del concesionario parra-arango cuya funcion es responder preguntas a los clientes.\n","Usa las siguientes piezas de información contextual para responder la pregunta. Si no conoces la respuesta, di que\n","esa información no la tienes disponible.\n","Manten una respuesta concisa\n","Question: {question} \n","\n","Context: {context} \n","\n","Answer:\"\"\"\n","prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system),\n","        (\"human\", \"{question}\"),\n","    ]\n",")\n","\n","# Post-processing\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","# Chain\n","rag_chain = prompt | llm #| StrOutputParser()\n","\n","# #Run \n","# generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n","# print(generation)\n","\n","### Hallucination Grader\n","\n","# Data model\n","class GradeHallucinations(BaseModel):\n","    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n","\n","    binary_score: str = Field(\n","        description=\"Answer is grounded in the facts, 'si' or 'no'\"\n","    )\n","\n","\n","# LLM with function call\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n","structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n","\n","# Prompt\n","system = \"\"\"Usted es un evaluador que está valorando si una generación de LLM está fundamentada en / apoyada por un conjunto de hechos recuperados.\n","Asigne una puntuación binaria 'si' o 'no'. 'si' significa que la respuesta está fundamentada en / apoyada por el conjunto de hechos.\"\"\"\n","hallucination_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system),\n","        (\"human\", \"Conjunto de hechos: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n","    ]\n",")\n","\n","hallucination_grader = hallucination_prompt | structured_llm_grader\n","#hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n","\n","### Answer Grader\n","\n","\n","# Data model\n","class GradeAnswer(BaseModel):\n","    \"\"\"Binary score to assess answer addresses question.\"\"\"\n","\n","    binary_score: str = Field(\n","        description=\"Answer addresses the question, 'si' or 'no'\"\n","    )\n","\n","\n","# LLM with function call\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n","structured_llm_grader = llm.with_structured_output(GradeAnswer)\n","\n","# Prompt\n","system = \"\"\"Usted es un evaluador que está valorando si una respuesta aborda / resuelve una pregunta.\n","Asigne una puntuación binaria 'si' o 'no'. 'si' significa que la respuesta resuelve la pregunta.\"\"\"\n","answer_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system),\n","        (\"human\", \"Pregunta del usuario: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n","    ]\n",")\n","\n","answer_grader = answer_prompt | structured_llm_grader\n","#answer_grader.invoke({\"question\": question, \"generation\": generation})\n","\n","### Question Re-writer\n","\n","llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n","\n","# Prompt\n","system = \"\"\"Usted es un reformulador de preguntas que convierte una pregunta de entrada en una versión mejorada y optimizada\n","para la recuperación de información en un vectorstore. Analice la entrada e intente razonar sobre la intención / significado semántico subyacente. \n","Tenga en cuenta el historial de mensajes del usuario para completar la pregunta, y mantenga el significado semantico subyacente\"\"\"\n","re_write_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system),\n","        (\n","            \"human\",\n","            \"Aqui esta la pregunta inicial: \\n\\n {question} \\n Formule una respuesta mejorada.\",\n","        ),\n","         (\"placeholder\",\"{messages}\")\n","    ]\n",")\n","\n","question_rewriter = re_write_prompt | llm | StrOutputParser()\n","#question_rewriter.invoke({\"question\": question})\n","\n","\n","\n","###Graph state\n","class GraphState(TypedDict):\n","    \"\"\"\n","    Represents the state of our graph.\n","\n","    Attributes:\n","        question: question\n","        generation: LLM generation\n","        documents: list of documents\n","    \"\"\"\n","\n","    question: str\n","    generation: str\n","    documents: List[str]\n","    messages : Annotated[list, add_messages]\n","\n","\n","### Nodes\n","def agent(state):\n","    \"\"\"\n","    Invokes the agent model to generate a response based on the current state. Given\n","    the input, it will decide to use any tool, retrieve info, or keep chatting.\n","\n","    Args:\n","        state (messages): The current state\n","\n","    Returns:\n","        dict: The updated state with the agent response appended to messages\n","    \"\"\"\n","    print(\"---CALL AGENT---\")\n","    message=state['messages']\n","    response=agent_router.invoke({\"input\":message})\n","    \n","\n","    return {\"messages\": [response]}\n","\n","     \n","\n","# def retrieve(state):\n","#     \"\"\"\n","#     Retrieve documents\n","\n","#     Args:\n","#         state (dict): The current graph state\n","\n","#     Returns:\n","#         state (dict): New key added to state, documents, that contains retrieved documents\n","#     \"\"\"\n","#     print(\"---RETRIEVE---\")\n","#     question = state[\"question\"]\n","\n","#     # Retrieval\n","#     documents = retriever.invoke(question)\n","#     return {\"documents\": documents, \"question\": question}\n","\n","\n","def generate(state):\n","    \"\"\"\n","    Generate answer\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        state (dict): New key added to state, generation, that contains LLM generation\n","    \"\"\"\n","    print(\"---GENERATE---\")\n","    question = state[\"question\"]\n","    documents = state[\"documents\"]\n","\n","    # RAG generation\n","    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n","    return {\"documents\": documents, \"question\": question, \"messages\": [generation],\"generation\":generation.content}\n","\n","\n","def grade_documents(state):\n","    \"\"\"\n","    Determines whether the retrieved documents are relevant to the question.\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        state (dict): Updates documents key with only filtered relevant documents\n","    \"\"\"\n","\n","    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n","    question = state[\"messages\"][-2].tool_calls[0]['args']['query']#penultimo mensaje es el AI message que se manda para la tool call\n","    raw_document = state[\"messages\"][-1].content #Ulimo mensaje siempte es lo que devuelve el retrieve tool\n","\n","    documents=raw_document.split(\"\\n\\n\\n\")\n","    filtered_documents =[]\n","    for document in documents:\n","        score = retrieval_grader.invoke(\n","        {\"question\": question, \"document\": document}\n","        )\n","        grade = score.binary_score\n","        if grade == \"si\":\n","            print(\"---GRADE: DOCUMENT RELEVANT---\")\n","            filtered_documents.append(document)\n","        else:\n","            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n","    return {\"documents\": filtered_documents, \"question\": question}\n","\n","\n","def transform_query(state):\n","    \"\"\"\n","    Transform the query to produce a better question.\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        state (dict): Updates question key with a re-phrased question\n","    \"\"\"\n","\n","    print(\"---TRANSFORM QUERY---\")\n","    question = state[\"question\"]\n","    documents = state[\"documents\"]\n","\n","    # Re-write question\n","    better_question = question_rewriter.invoke({\"question\": question,\"messages\":state[\"messages\"]})\n","    retrieve_call=agent_router.invoke({\"input\":better_question})\n","    return {\"documents\": documents, \"question\": better_question,\"messages\":[retrieve_call]}\n","\n","\n","### Edges ###\n","\n","\n","\n","def decide_to_generate(state):\n","    \"\"\"\n","    Determines whether to generate an answer, or re-generate a question.\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        str: Binary decision for next node to call\n","    \"\"\"\n","\n","    print(\"---ASSESS GRADED DOCUMENTS---\")\n","    state[\"question\"]\n","    filtered_documents = state[\"documents\"]\n","\n","    if not filtered_documents:\n","        # All documents have been filtered check_relevance\n","        # We will re-generate a new query\n","        print(\n","            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n","        )\n","        return \"transform_query\"\n","    else:\n","        # We have relevant documents, so generate answer\n","        print(\"---DECISION: GENERATE---\")\n","        return \"generate\"\n","\n","\n","def grade_generation_v_documents_and_question(state):\n","    \"\"\"\n","    Determines whether the generation is grounded in the document and answers question.\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        str: Decision for next node to call\n","    \"\"\"\n","\n","    print(\"---CHECK HALLUCINATIONS---\")\n","    question = state[\"question\"]\n","    documents = state[\"documents\"]\n","    generation = state[\"generation\"]\n","\n","    score = hallucination_grader.invoke(\n","        {\"documents\": documents, \"generation\": generation}\n","    )\n","    grade = score.binary_score\n","\n","    # Check hallucination\n","    if grade == \"si\":\n","        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n","        # Check question-answering\n","        print(\"---GRADE GENERATION vs QUESTION---\")\n","        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n","        grade = score.binary_score\n","        if grade == \"si\":\n","            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n","            return \"useful\"\n","        else:\n","            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n","            return \"not useful\"\n","    else:\n","        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n","        return \"not supported\"\n","\n","#Define tool routing\n","\n","def tools_condition_modified(\n","    state: Union[list[AnyMessage], dict[str, Any]],\n",") -> Literal[\"tools\", \"__end__\"]:\n","    \"\"\"Use in the conditional_edge to route to the ToolNode if the last message\n","\n","    has tool calls. Otherwise, route to the end.\n","\n","    Args:\n","        state (Union[list[AnyMessage], dict[str, Any]]): The state to check for\n","            tool calls. Must have a list of messages (MessageGraph) or have the\n","            \"messages\" key (StateGraph).\n","\n","    Returns:\n","        The next node to route to.\n","    \"\"\"\n","    if isinstance(state, list):\n","        ai_message = state[-1]\n","    elif messages := state.get(\"messages\", []):\n","        ai_message = messages[-1]\n","    else:\n","        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n","    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n","        for tool_call in ai_message.tool_calls:\n","            if tool_call[\"name\"]==\"retrieve_info\":\n","                return \"retrieve\"\n","        return \"tools\"\n","    return \"__end__\""]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["\n","workflow = StateGraph(GraphState)\n","\n","# Define the nodes\n","workflow.add_node(\"agent\",agent)\n","retrieve=ToolNode([retriever_tool])\n","workflow.add_node(\"retrieve\", retrieve)  # retrieve\n","tool_node=ToolNode(tools)\n","workflow.add_node(\"tools\",tool_node)\n","workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n","workflow.add_node(\"generate\", generate)  # generatae\n","workflow.add_node(\"transform_query\", transform_query)  # transform_query\n","\n","# Build graph\n","workflow.add_edge(START,\"agent\")\n","workflow.add_conditional_edges(\n","    \"agent\",\n","    tools_condition_modified,\n","    {\n","        \"retrieve\": \"retrieve\",\n","        \"tools\":\"tools\",\n","        END:END\n","    },\n",")\n","workflow.add_edge(\"tools\",\"agent\")\n","workflow.add_edge(\"retrieve\", \"grade_documents\")\n","workflow.add_conditional_edges(\n","    \"grade_documents\",\n","    decide_to_generate,\n","    {\n","        \"transform_query\": \"transform_query\",\n","        \"generate\": \"generate\",\n","    },\n",")\n","workflow.add_edge(\"transform_query\", \"retrieve\")\n","workflow.add_conditional_edges(\n","    \"generate\",\n","    grade_generation_v_documents_and_question,\n","    {\n","        \"not supported\": \"generate\",\n","        \"useful\": END,\n","        \"not useful\": \"transform_query\",\n","    },\n",")\n","\n","# Compile\n","app = workflow.compile(checkpointer=memory)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---CALL AGENT---\n","\"Node 'agent':\"\n","{ 'messages': [ AIMessage(content='Hola, ¿en qué puedo ayudarte hoy?', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 264, 'total_tokens': 274}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_3aa7262c27', 'finish_reason': 'stop', 'logprobs': None}, id='run-f56fd840-0708-4a65-9ae8-f7810b5277d1-0')]}\n","'\\n---\\n'\n"]},{"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m config \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mconfigurable\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mthread_id\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m6\u001b[39m\u001b[39m\"\u001b[39m}}\n\u001b[0;32m      3\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     user_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m()\n\u001b[0;32m      5\u001b[0m     \u001b[39mif\u001b[39;00m user_input\u001b[39m.\u001b[39mlower() \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mquit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mexit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mq\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m      6\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGoodbye!\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\chatbot\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/chatbot/Lib/site-packages/ipykernel/kernelbase.py?line=1279'>1280</a>\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/chatbot/Lib/site-packages/ipykernel/kernelbase.py?line=1280'>1281</a>\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/chatbot/Lib/site-packages/ipykernel/kernelbase.py?line=1281'>1282</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/chatbot/Lib/site-packages/ipykernel/kernelbase.py?line=1282'>1283</a>\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/chatbot/Lib/site-packages/ipykernel/kernelbase.py?line=1283'>1284</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/chatbot/Lib/site-packages/ipykernel/kernelbase.py?line=1284'>1285</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/chatbot/Lib/site-packages/ipykernel/kernelbase.py?line=1285'>1286</a>\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/chatbot/Lib/site-packages/ipykernel/kernelbase.py?line=1286'>1287</a>\u001b[0m )\n","File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\chatbot\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/chatbot/Lib/site-packages/ipykernel/kernelbase.py?line=1321'>1322</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/chatbot/Lib/site-packages/ipykernel/kernelbase.py?line=1322'>1323</a>\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/chatbot/Lib/site-packages/ipykernel/kernelbase.py?line=1323'>1324</a>\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/chatbot/Lib/site-packages/ipykernel/kernelbase.py?line=1324'>1325</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/chatbot/Lib/site-packages/ipykernel/kernelbase.py?line=1325'>1326</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/chatbot/Lib/site-packages/ipykernel/kernelbase.py?line=1326'>1327</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}],"source":["config = {\"configurable\": {\"thread_id\": \"6\"}}\n","\n","while True:\n","    user_input = input()\n","    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n","        print(\"Goodbye!\")\n","        break\n","    for event in app.stream({\"messages\": [user_input]},config=config):\n","        for key,value in event.items():\n","            # Node\n","            pprint(f\"Node '{key}':\")\n","            # Optional: print full state at each node\n","            pprint(value, indent=2, width=80, depth=None)\n","        pprint(\"\\n---\\n\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.12.3 ('chatbot')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"8b9d66280c6aa0c9920c23978bcee7765451a6748ab211777099794f360f1c01"}}},"nbformat":4,"nbformat_minor":2}
